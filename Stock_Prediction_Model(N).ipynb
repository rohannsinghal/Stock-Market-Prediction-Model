{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631baee6-facb-4516-b943-408b9364fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Stock N data points: 116\n",
      "Date range: 2022-07-18 00:00:00 to 2022-12-30 00:00:00\n",
      "Missing values in Stock N:\n",
      "Series([], dtype: int64)\n",
      "All missing values in Stock N data have been handled.\n",
      "Using 22 features for Stock N model\n",
      "Starting LSTM model training for Stock N...\n",
      "Training sequences: 84\n",
      "Validation sequences: 10\n",
      "Testing sequences: 12\n",
      "Scalers saved to: /Users/admin/Downloads/stock_N_scalers_20250327_144425.pkl\n",
      "Training LSTM model for Stock N...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 14:44:26.074666: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-03-27 14:44:26.074846: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-03-27 14:44:26.074852: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-03-27 14:44:26.075189: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-27 14:44:26.075206: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 14:44:29.023178: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-03-27 14:44:29.029979: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.1110\n",
      "Epoch 1: val_loss improved from inf to 0.07369, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 250ms/step - loss: 0.1075 - val_loss: 0.0737\n",
      "Epoch 2/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0323\n",
      "Epoch 2: val_loss did not improve from 0.07369\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0306 - val_loss: 0.0869\n",
      "Epoch 3/200\n",
      "\u001b[1m 9/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0117\n",
      "Epoch 3: val_loss improved from 0.07369 to 0.05577, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0122 - val_loss: 0.0558\n",
      "Epoch 4/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0113\n",
      "Epoch 4: val_loss did not improve from 0.05577\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0115 - val_loss: 0.0937\n",
      "Epoch 5/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0114\n",
      "Epoch 5: val_loss improved from 0.05577 to 0.05040, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0113 - val_loss: 0.0504\n",
      "Epoch 6/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0096\n",
      "Epoch 6: val_loss did not improve from 0.05040\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0100 - val_loss: 0.0753\n",
      "Epoch 7/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0103\n",
      "Epoch 7: val_loss improved from 0.05040 to 0.04669, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0101 - val_loss: 0.0467\n",
      "Epoch 8/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0122\n",
      "Epoch 8: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0120 - val_loss: 0.0555\n",
      "Epoch 9/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0096\n",
      "Epoch 9: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0098 - val_loss: 0.0682\n",
      "Epoch 10/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0076\n",
      "Epoch 10: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0076 - val_loss: 0.0578\n",
      "Epoch 11/200\n",
      "\u001b[1m 9/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0065\n",
      "Epoch 11: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0068 - val_loss: 0.0565\n",
      "Epoch 12/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0070\n",
      "Epoch 12: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0071 - val_loss: 0.0498\n",
      "Epoch 13/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0071\n",
      "Epoch 13: val_loss did not improve from 0.04669\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0071 - val_loss: 0.0598\n",
      "Epoch 14/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0060\n",
      "Epoch 14: val_loss improved from 0.04669 to 0.04500, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0061 - val_loss: 0.0450\n",
      "Epoch 15/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0092\n",
      "Epoch 15: val_loss did not improve from 0.04500\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0091 - val_loss: 0.0645\n",
      "Epoch 16/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0069\n",
      "Epoch 16: val_loss improved from 0.04500 to 0.03683, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0070 - val_loss: 0.0368\n",
      "Epoch 17/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0099\n",
      "Epoch 17: val_loss did not improve from 0.03683\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0096 - val_loss: 0.0589\n",
      "Epoch 18/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0066\n",
      "Epoch 18: val_loss did not improve from 0.03683\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0066 - val_loss: 0.0532\n",
      "Epoch 19/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0049\n",
      "Epoch 19: val_loss did not improve from 0.03683\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0051 - val_loss: 0.0407\n",
      "Epoch 20/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0042\n",
      "Epoch 20: val_loss did not improve from 0.03683\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0044 - val_loss: 0.0495\n",
      "Epoch 21/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0103\n",
      "Epoch 21: val_loss improved from 0.03683 to 0.02904, saving model to /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0100 - val_loss: 0.0290\n",
      "Epoch 22/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0065\n",
      "Epoch 22: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0066 - val_loss: 0.0637\n",
      "Epoch 23/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0081\n",
      "Epoch 23: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0079 - val_loss: 0.0326\n",
      "Epoch 24/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0054\n",
      "Epoch 24: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0054 - val_loss: 0.0360\n",
      "Epoch 25/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069\n",
      "Epoch 25: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0071 - val_loss: 0.0418\n",
      "Epoch 26/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0052\n",
      "Epoch 26: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0054 - val_loss: 0.0447\n",
      "Epoch 27/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0044\n",
      "Epoch 27: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0045 - val_loss: 0.0535\n",
      "Epoch 28/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0060\n",
      "Epoch 28: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0061 - val_loss: 0.0356\n",
      "Epoch 29/200\n",
      "\u001b[1m 9/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0055\n",
      "Epoch 29: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0056 - val_loss: 0.0462\n",
      "Epoch 30/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0049\n",
      "Epoch 30: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0047 - val_loss: 0.0370\n",
      "Epoch 31/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0064\n",
      "Epoch 31: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0063 - val_loss: 0.0488\n",
      "Epoch 32/200\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0051\n",
      "Epoch 32: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0051 - val_loss: 0.0401\n",
      "Epoch 33/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0045\n",
      "Epoch 33: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - val_loss: 0.0464\n",
      "Epoch 34/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0100\n",
      "Epoch 34: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0097 - val_loss: 0.0388\n",
      "Epoch 35/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0067\n",
      "Epoch 35: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0065 - val_loss: 0.0314\n",
      "Epoch 36/200\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0069\n",
      "Epoch 36: val_loss did not improve from 0.02904\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0067 - val_loss: 0.0416\n",
      "Evaluating model...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
      "\n",
      "Performance metrics for Stock N:\n",
      "MSE: 408.19\n",
      "RMSE: 20.20\n",
      "MAE: 19.40\n",
      "MAPE: 6.21%\n",
      "R^2: -16.65\n",
      "Generating predictions for 2023...\n",
      "Generated 250 trading days for 2023\n",
      "Predicting for 2023-12-29 (250/250)\n",
      "Predictions saved to /Users/admin/Downloads/stock_N_predictions_2023_20250327_144425.csv\n",
      "\n",
      "Model training and prediction for Stock N complete!\n",
      "Model saved to: /Users/admin/Downloads/lstm_stock_N_model_20250327_144425.h5\n",
      "Predictions saved to: /Users/admin/Downloads/stock_N_predictions_2023_20250327_144425.csv\n",
      "Created new Excel file with Stock N predictions at /Users/admin/Downloads/final_submission_predictions_2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import joblib\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names\")\n",
    "\n",
    "# File paths\n",
    "file_path = 'final_featured_stock_data_cleaned.csv'\n",
    "save_dir = '/Users/admin/Downloads'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Timestamp for model and outputs\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = os.path.join(save_dir, f'lstm_stock_N_model_{timestamp}.h5')\n",
    "scaler_path = os.path.join(save_dir, f'stock_N_scalers_{timestamp}.pkl')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(file_path)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract only Stock N data\n",
    "stock_n_data = df[df['Stock_ID'] == 'Stock N'].copy()\n",
    "print(f\"Stock N data points: {len(stock_n_data)}\")\n",
    "print(f\"Date range: {stock_n_data['Date'].min()} to {stock_n_data['Date'].max()}\")\n",
    "\n",
    "# Check for missing values in Stock N data\n",
    "missing_values = stock_n_data.isna().sum()\n",
    "print(\"Missing values in Stock N:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Handle missing values\n",
    "for col in stock_n_data.columns:\n",
    "    if stock_n_data[col].isna().sum() > 0:\n",
    "        if col in ['SMA_10', 'EMA_10', 'RSI_14', 'MACD', 'Signal_Line', 'OBV', 'Rolling_Mean_20', 'Rolling_Std_20', 'Bollinger_Upper', 'Bollinger_Lower']:\n",
    "            # For technical indicators, forward fill is appropriate\n",
    "            stock_n_data[col] = stock_n_data[col].fillna(method='ffill')\n",
    "            # For any remaining NaNs at the beginning, backward fill\n",
    "            stock_n_data[col] = stock_n_data[col].fillna(method='bfill')\n",
    "        else:\n",
    "            # For lag and rolling features\n",
    "            stock_n_data[col] = stock_n_data[col].fillna(method='ffill')\n",
    "            stock_n_data[col] = stock_n_data[col].fillna(method='bfill')\n",
    "\n",
    "# Verify all missing values are handled\n",
    "if stock_n_data.isna().sum().sum() > 0:\n",
    "    print(\"Remaining missing values after cleaning:\")\n",
    "    print(stock_n_data.isna().sum()[stock_n_data.isna().sum() > 0])\n",
    "else:\n",
    "    print(\"All missing values in Stock N data have been handled.\")\n",
    "\n",
    "# Select feature columns and target\n",
    "exclude_cols = ['Date', 'Stock_ID', 'Year', 'Month', 'Day', 'Day_of_Week', 'Quarter', 'time_idx']\n",
    "feature_cols = [col for col in stock_n_data.columns if col not in exclude_cols and col != 'Close']\n",
    "target_col = 'Close'\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features for Stock N model\")\n",
    "\n",
    "# Sort by date\n",
    "stock_n_data = stock_n_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Sequence parameters - adjusted for Stock N's smaller dataset\n",
    "sequence_length = 10  # Shorter sequence due to limited data\n",
    "prediction_days = 1   # Predict one day at a time\n",
    "\n",
    "# Function to prepare data for LSTM\n",
    "def prepare_data(data, feature_cols, target_col, sequence_length, prediction_days):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): DataFrame containing stock data\n",
    "    feature_cols (list): List of feature column names\n",
    "    target_col (str): Target column name\n",
    "    sequence_length (int): Number of past time steps to use\n",
    "    prediction_days (int): Number of days to predict ahead\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, y_train, X_val, y_val, X_test, y_test, scalers)\n",
    "    \"\"\"\n",
    "    # Create scalers for features and target\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Fit and transform feature data\n",
    "    feature_data = feature_scaler.fit_transform(data[feature_cols])\n",
    "    \n",
    "    # Fit and transform target data\n",
    "    target_data = target_scaler.fit_transform(data[[target_col]])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(feature_data) - sequence_length - prediction_days + 1):\n",
    "        X.append(feature_data[i:(i + sequence_length)])\n",
    "        y.append(target_data[i + sequence_length + prediction_days - 1])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train (70%), validation (15%), and test (15%)\n",
    "    # For small datasets, we can use a higher percentage for training\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    val_size = int(len(X) * 0.1)\n",
    "    \n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "    \n",
    "    print(f\"Training sequences: {len(X_train)}\")\n",
    "    print(f\"Validation sequences: {len(X_val)}\")\n",
    "    print(f\"Testing sequences: {len(X_test)}\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, (feature_scaler, target_scaler)\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_lstm_model(X_train, y_train, X_val, y_val, save_path):\n",
    "    \"\"\"\n",
    "    Build and train LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array): Training features\n",
    "    y_train (array): Training target\n",
    "    X_val (array): Validation features\n",
    "    y_val (array): Validation target\n",
    "    save_path (str): Path to save the model\n",
    "    \n",
    "    Returns:\n",
    "    model: Trained LSTM model\n",
    "    history: Training history\n",
    "    \"\"\"\n",
    "    # Define model architecture - simpler for smaller dataset\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Model checkpoint\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=save_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model - more epochs for smaller dataset\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=8,  # Smaller batch size for smaller dataset\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_test, y_test, target_scaler):\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    X_test (array): Test features\n",
    "    y_test (array): Test target\n",
    "    target_scaler: Scaler used for target variable\n",
    "    \n",
    "    Returns:\n",
    "    dict: Performance metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions and actual values\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test)\n",
    "    y_pred_inv = target_scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test_inv - y_pred_inv) / y_test_inv)) * 100\n",
    "    \n",
    "    print(f\"\\nPerformance metrics for Stock N:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"R^2: {r2:.2f}\")\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_inv, label='Actual')\n",
    "    plt.plot(y_pred_inv, label='Predicted')\n",
    "    plt.title('Stock N - Actual vs Predicted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = os.path.join(save_dir, f'stock_N_test_predictions_{timestamp}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Function to generate predictions for 2023\n",
    "def generate_2023_predictions(model, feature_scaler, target_scaler, latest_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Generate predictions for all trading days in 2023\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    feature_scaler: Scaler for features\n",
    "    target_scaler: Scaler for target\n",
    "    latest_data (DataFrame): Latest available data\n",
    "    feature_cols (list): Feature columns\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Predictions for 2023\n",
    "    \"\"\"\n",
    "    # Generate all trading days for 2023\n",
    "    def generate_2023_trading_days():\n",
    "        \"\"\"Generate all trading days (Mon-Fri) for 2023\"\"\"\n",
    "        start_date = pd.Timestamp('2023-01-01')\n",
    "        end_date = pd.Timestamp('2023-12-31')\n",
    "        \n",
    "        # Generate all days\n",
    "        all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        \n",
    "        # Filter to only include weekdays (Monday=0, Sunday=6)\n",
    "        trading_days = [day for day in all_days if day.weekday() < 5]\n",
    "        \n",
    "        # US Market Holidays 2023\n",
    "        holidays_2023 = [\n",
    "            '2023-01-02',  # New Year's Day (observed)\n",
    "            '2023-01-16',  # Martin Luther King Jr. Day\n",
    "            '2023-02-20',  # Presidents' Day\n",
    "            '2023-04-07',  # Good Friday\n",
    "            '2023-05-29',  # Memorial Day\n",
    "            '2023-06-19',  # Juneteenth\n",
    "            '2023-07-04',  # Independence Day\n",
    "            '2023-09-04',  # Labor Day\n",
    "            '2023-11-23',  # Thanksgiving Day\n",
    "            '2023-12-25',  # Christmas Day\n",
    "        ]\n",
    "        \n",
    "        # Remove holidays\n",
    "        trading_days = [day for day in trading_days if day.strftime('%Y-%m-%d') not in holidays_2023]\n",
    "        \n",
    "        return trading_days\n",
    "    \n",
    "    trading_days = generate_2023_trading_days()\n",
    "    print(f\"Generated {len(trading_days)} trading days for 2023\")\n",
    "    \n",
    "    # Get the latest sequence from the data\n",
    "    extended_data = latest_data.copy()\n",
    "    \n",
    "    # Initialize predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # For each trading day\n",
    "    for i, future_date in enumerate(trading_days):\n",
    "        print(f\"Predicting for {future_date.strftime('%Y-%m-%d')} ({i+1}/{len(trading_days)})\", end='\\r')\n",
    "        \n",
    "        # Get the latest sequence\n",
    "        latest_sequence = extended_data.tail(sequence_length)\n",
    "        \n",
    "        # Extract features\n",
    "        X = latest_sequence[feature_cols].values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = feature_scaler.transform(X)\n",
    "        \n",
    "        # Reshape for LSTM [samples, time steps, features]\n",
    "        X_scaled = X_scaled.reshape(1, sequence_length, len(feature_cols))\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred_scaled = model.predict(X_scaled, verbose=0)\n",
    "        \n",
    "        # Inverse transform prediction\n",
    "        y_pred = target_scaler.inverse_transform(y_pred_scaled)[0, 0]\n",
    "        \n",
    "        # Store prediction\n",
    "        predictions.append({\n",
    "            'Date': future_date,\n",
    "            'Close': y_pred\n",
    "        })\n",
    "        \n",
    "        # Create a new row with the prediction\n",
    "        new_row = {\n",
    "            'Date': future_date,\n",
    "            'Close': y_pred,\n",
    "            'Stock_ID': 'Stock N'\n",
    "        }\n",
    "        \n",
    "        # Set default values for other columns\n",
    "        for col in extended_data.columns:\n",
    "            if col not in new_row:\n",
    "                if col in ['Year', 'Month', 'Day']:\n",
    "                    # Extract date components\n",
    "                    if col == 'Year':\n",
    "                        new_row[col] = future_date.year\n",
    "                    elif col == 'Month':\n",
    "                        new_row[col] = future_date.month\n",
    "                    elif col == 'Day':\n",
    "                        new_row[col] = future_date.day\n",
    "                elif col == 'Day_of_Week':\n",
    "                    new_row[col] = future_date.weekday()\n",
    "                elif col == 'Quarter':\n",
    "                    new_row[col] = (future_date.month - 1) // 3 + 1\n",
    "                elif col == 'time_idx':\n",
    "                    new_row[col] = extended_data['time_idx'].iloc[-1] + 1\n",
    "                elif col in ['Open', 'High', 'Low']:\n",
    "                    # For price columns, use the predicted Close\n",
    "                    new_row[col] = y_pred\n",
    "                elif col == 'Volume':\n",
    "                    # For Volume, use the average of last 5 days\n",
    "                    new_row[col] = extended_data.tail(5)['Volume'].mean()\n",
    "                else:\n",
    "                    # For other columns, use the last value\n",
    "                    new_row[col] = extended_data[col].iloc[-1]\n",
    "        \n",
    "        # Append the new row\n",
    "        extended_data = pd.concat([extended_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Create DataFrame with predictions\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    csv_path = os.path.join(save_dir, f'stock_N_predictions_2023_{timestamp}.csv')\n",
    "    predictions_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nPredictions saved to {csv_path}\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting LSTM model training for Stock N...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, scalers = prepare_data(\n",
    "        stock_n_data, feature_cols, target_col, sequence_length, prediction_days)\n",
    "    \n",
    "    feature_scaler, target_scaler = scalers\n",
    "    \n",
    "    # Save scalers\n",
    "    joblib.dump(scalers, scaler_path)\n",
    "    print(f\"Scalers saved to: {scaler_path}\")\n",
    "    \n",
    "    # Build and train model\n",
    "    print(\"Training LSTM model for Stock N...\")\n",
    "    model, history = build_lstm_model(X_train, y_train, X_val, y_val, model_path)\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_path = os.path.join(save_dir, f'stock_N_training_history_{timestamp}.csv')\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Stock N Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    loss_plot_path = os.path.join(save_dir, f'stock_N_training_loss_{timestamp}.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, target_scaler)\n",
    "    \n",
    "    # Generate predictions for 2023\n",
    "    print(\"Generating predictions for 2023...\")\n",
    "    predictions_df = generate_2023_predictions(\n",
    "        model, feature_scaler, target_scaler, stock_n_data, feature_cols)\n",
    "    \n",
    "    print(\"\\nModel training and prediction for Stock N complete!\")\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Predictions saved to: {os.path.join(save_dir, f'stock_N_predictions_2023_{timestamp}.csv')}\")\n",
    "    \n",
    "    # Add predictions to the final Excel file\n",
    "    final_excel_path = '/Users/admin/Downloads/final_submission_predictions_2023.xlsx'\n",
    "    \n",
    "    try:\n",
    "        # Load existing Excel file\n",
    "        book = pd.ExcelFile(final_excel_path)\n",
    "        with pd.ExcelWriter(final_excel_path, engine='openpyxl', mode='a') as writer:\n",
    "            # Format date as string before writing\n",
    "            predictions_df['Date'] = predictions_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            predictions_df.to_excel(writer, sheet_name='Stock N', index=False)\n",
    "        print(f\"Stock N predictions added to {final_excel_path}\")\n",
    "    except FileNotFoundError:\n",
    "        # If file doesn't exist, create a new one with just Stock N\n",
    "        with pd.ExcelWriter(final_excel_path, engine='openpyxl') as writer:\n",
    "            predictions_df['Date'] = predictions_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            predictions_df.to_excel(writer, sheet_name='Stock N', index=False)\n",
    "        print(f\"Created new Excel file with Stock N predictions at {final_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99f154-776e-4334-9248-fcddfa991d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
